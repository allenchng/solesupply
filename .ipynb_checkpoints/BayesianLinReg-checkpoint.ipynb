{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pystan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import diagnostics\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import density_intervals as di\n",
    "import seaborn as sns\n",
    "import engineer_features\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../big_data/sneaker_transactions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = engineer_features.make_data(path, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(path, days_behind, days_forward):\n",
    "    model_df = Engineer_features(path)\n",
    "    \n",
    "    cat_bool = ['day_of_week', 'month', 'year', 'holiday', 'date', 'is_retro', 'is_yeezy', 'is_pharrell']\n",
    "    cat_df = model_df.loc[:, (cat_bool)]\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    newdf = model_df.select_dtypes(include=numerics)\n",
    "    \n",
    "    tmp = newdf.loc[:, ('volume', 'rolling_mean_week', 'rolling_median_week', \n",
    "                    'rolling_max_week', 'projected_volume')]\n",
    "\n",
    "    scaler = StandardScaler() \n",
    "    scaled_values = scaler.fit_transform(tmp) \n",
    "    tmp.loc[:,:] = scaled_values\n",
    "    \n",
    "    model_df = pd.concat([cat_df, tmp], axis=1, sort=False)\n",
    "    model_df.date = pd.to_datetime(model_df.date)\n",
    "    \n",
    "    model_df = model_df[model_df['date'] > '2017-10-30']\n",
    "    data = model_df.drop(\"date\", axis=1)\n",
    "    \n",
    "    rf_df = pd.get_dummies(data)\n",
    "    rf_df = rf_df.dropna()\n",
    "    rf_df = rf_df.reset_index(drop = True)\n",
    "\n",
    "    labels = np.array(rf_df['projected_volume'])\n",
    "\n",
    "    # Remove the labels from the features\n",
    "    features= rf_df.drop('projected_volume', axis = 1)\n",
    "\n",
    "    # Saving feature names for later use\n",
    "    feature_list = list(features.columns)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    features = np.array(features)\n",
    "    \n",
    "    split_axis = round(len(model_df) * train_pct)\n",
    "    \n",
    "    train = rf_df[:split_axis]\n",
    "    test = rf_df[split_axis:-6]\n",
    "\n",
    "    train_labels = np.array(train['projected_volume'])\n",
    "    train_features = train.drop('projected_volume', axis = 1)\n",
    "\n",
    "    test_labels = np.array(test['projected_volume'])\n",
    "    test_features = test.drop('projected_volume', axis = 1)\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = EngineerFeatures(daily_volume, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_bool = ['day_of_week', 'month', 'year', 'holiday', 'date', 'is_retro', 'is_yeezy', 'is_pharrell']\n",
    "cat_df = model_df.loc[:, (cat_bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "newdf = model_df.select_dtypes(include=numerics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = newdf.loc[:, ('volume', 'rolling_mean_week', 'rolling_median_week', \n",
    "                    'rolling_max_week', 'projected_volume')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "scaled_values = scaler.fit_transform(tmp) \n",
    "tmp.loc[:,:] = scaled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.concat([cat_df, tmp], axis=1, sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.date = pd.to_datetime(model_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(model_df, train_pct):\n",
    "    \n",
    "    model_df = model_df[model_df['date'] > '2017-10-30']\n",
    "    data = model_df.drop(\"date\", axis=1)\n",
    "    \n",
    "    rf_df = pd.get_dummies(data)\n",
    "    rf_df = rf_df.dropna()\n",
    "    rf_df = rf_df.reset_index(drop = True)\n",
    "\n",
    "    # Labels are the values we want to predict\n",
    "    labels = np.array(rf_df['projected_volume'])\n",
    "\n",
    "    # Remove the labels from the features\n",
    "    # axis 1 refers to the columns\n",
    "    features= rf_df.drop('projected_volume', axis = 1)\n",
    "\n",
    "    # Saving feature names for later use\n",
    "    feature_list = list(features.columns)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    features = np.array(features)\n",
    "    \n",
    "    split_axis = round(len(model_df) * train_pct)\n",
    "    \n",
    "    train = rf_df[:split_axis]\n",
    "    test = rf_df[split_axis:-6]\n",
    "\n",
    "    train_labels = np.array(train['projected_volume'])\n",
    "    train_features = train.drop('projected_volume', axis = 1)\n",
    "\n",
    "    test_labels = np.array(test['projected_volume'])\n",
    "    test_features = test.drop('projected_volume', axis = 1)\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels, test_features, test_labels = split_train_test(model_df, train_pct = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_normal = \"\"\"\n",
    "    /* Spec for robust linear regression model */\n",
    "    data {                                 \n",
    "        int<lower=0> N;                     // count of observations\n",
    "        int<lower=0> K;                     // count of features\n",
    "        matrix[N, K] X;                     // feature matrix\n",
    "        real y[N];          // target\n",
    "    }\n",
    "    parameters {\n",
    "        real alpha;                         // constant\n",
    "        vector[K] beta;                     // feature coefficients\n",
    "        real<lower=0> sigma;\n",
    "    }\n",
    "    model { \n",
    "        alpha ~ normal(0,3);              // prior\n",
    "        beta ~ normal(0,3);              // prior\n",
    "        y ~ normal(alpha + X * beta, sigma);         // likelihood\n",
    "    }\n",
    "    generated quantities {}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Train a Bayesian linear model.\n",
    "\n",
    "    Args:\n",
    "        train: A dataframe of training data.\n",
    "        labels: A numpy array of labels.\n",
    "\n",
    "    Returns:\n",
    "        fit: A fitted Stan model.  \n",
    "    \"\"\"\n",
    "    \n",
    "    features, labels = input_fn()\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    stan_datadict = {}\n",
    "    stan_datadict['N'] = train_features.shape[0]\n",
    "    stan_datadict['K'] = train_features.shape[1]\n",
    "    stan_datadict['X'] = train_features.values\n",
    "    stan_datadict['y'] = train_labels\n",
    "\n",
    "    model = pystan.StanModel(model_code=lin_reg_normal)\n",
    "\n",
    "    fit = model.sampling(data=stan_datadict,\n",
    "                    warmup=250,\n",
    "                    iter = 1000, \n",
    "                    verbose = True,\n",
    "                    control={'max_treedepth': 15})\n",
    "    \n",
    "    end = time.time()\n",
    "    print('Time to fit model ' + str(end-start) + ' seconds.')\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stan_datadict = {}\n",
    "stan_datadict['N'] = train_features.shape[0]\n",
    "stan_datadict['K'] = train_features.shape[1]\n",
    "stan_datadict['X'] = train_features.values\n",
    "stan_datadict['y'] = train_labels\n",
    "\n",
    "model = pystan.StanModel(model_code=lin_reg_normal)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fit = model.sampling(data=stan_datadict,\n",
    "                warmup=250,\n",
    "                iter = 1000, \n",
    "                verbose = True,\n",
    "                control={'max_treedepth': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose(fit):\n",
    "    \n",
    "    \"\"\"Visualize the posterior distribution of model features.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        fit: A fitted Stan model.\n",
    "        selected_features: A list of strings indicating which features to visualize.\n",
    "\n",
    "    Returns:\n",
    "        Saves PNG's of the posterior distribution for all the weights and a selected \n",
    "    \"\"\"\n",
    "    \n",
    "    print(diagnostics.check_treedepth(fit))\n",
    "    print(diagnostics.check_energy(fit))\n",
    "    print(diagnostics.check_div(fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(fit):\n",
    "    \n",
    "    \"\"\"Visualize the posterior distribution of model features.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        fit: A fitted Stan model.\n",
    "        selected_features: A list of strings indicating which features to visualize.\n",
    "\n",
    "    Returns:\n",
    "        Saves PNG's of the posterior distribution for all the weights and a selected \n",
    "    \"\"\"\n",
    "    \n",
    "    features, labels = input_fn()\n",
    "    \n",
    "    b = fit.extract(['alpha'])['alpha'].mean()\n",
    "    w = fit.extract(['beta'])['beta'].mean(axis=0)\n",
    "\n",
    "    predictions = test_features@w + b\n",
    "    \n",
    "    numer_test_features = test_features.loc[:, ('volume', 'rolling_mean_week', 'rolling_median_week', \n",
    "                    'rolling_max_week')]\n",
    "    \n",
    "    unscale_test = pd.concat([numer_test_features, predictions],axis=1)\n",
    "    unscale_test.columns = ['volume', 'rolling_mean_week', 'rolling_median_week', 'rolling_max_week', 'projected_volume']\n",
    "    unscaled_predictions = scaler.inverse_transform(unscale_test)\n",
    "    unscale_test.loc[:,:] = unscaled_predictions\n",
    "    unscaled_predictions = unscale_test.projected_volume\n",
    "    \n",
    "    test_labels_df = pd.Series(test_labels)\n",
    "    test_labels_df.index = numer_test_features.index\n",
    "    unscale_labels = pd.concat([numer_test_features, test_labels_df],axis=1)\n",
    "    unscale_labels.columns = ['volume', 'rolling_mean_week', 'rolling_median_week', 'rolling_max_week', 'projected_volume']\n",
    "\n",
    "    unscaled_test_labels = scaler.inverse_transform(unscale_labels)\n",
    "    unscale_labels.loc[:,:] = unscaled_test_labels\n",
    "    \n",
    "    observed = unscale_labels.projected_volume\n",
    "    persistance_prediction = unscale_labels.volume\n",
    "    \n",
    "    print('Mean Absolute Percentage Error - Baseline:', round(np.mean(np.abs((observed - persistance_prediction) / observed)) * 100))\n",
    "    print('Mean Absolute Percentage Error - Bayes:', round(np.mean(np.abs((observed - unscaled_predictions) / observed)) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(fit):\n",
    "    \n",
    "    \"\"\"Visualize the posterior distribution of model features.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        fit: A fitted Stan model.\n",
    "        selected_features: A list of strings indicating which features to visualize.\n",
    "\n",
    "    Returns:\n",
    "        Saves PNG's of the posterior distribution for all the weights and a selected \n",
    "    \"\"\"\n",
    "    \n",
    "    samples = fit.extract()\n",
    "    beta = samples['beta']\n",
    "    \n",
    "    col_names = test_features.columns\n",
    "    weights = pd.DataFrame(beta)\n",
    "    weights.columns = col_names\n",
    "    melted_weights = weights.melt()\n",
    "    \n",
    "    # Visualize and save posteriors for all predictors\n",
    "    a = sns.FacetGrid(melted_weights, col='variable', col_wrap=5, sharex=False, sharey=False)\n",
    "    a = a.map(sns.distplot, \"value\")\n",
    "    a.savefig('./figures/all_variable_posteriors.png', dpi = 400)\n",
    "    \n",
    "    # Visualize and save posteriors for selected features\n",
    "    desired_features = ['volume', 'rolling_mean_week', 'is_retro', 'is_yeezy']\n",
    "    posterior_features = melted_weights[melted_weights['variable'].isin(desired_features)]\n",
    "    g = sns.FacetGrid(posterior_features, col='variable', sharex=False, sharey=False)\n",
    "    g = g.map(sns.distplot, \"value\")\n",
    "    g.savefig('./figures/selected_variable_posteriors.png', dpi = 400)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465 of 3000 iterations saturated the maximum tree depth of 10 (15.5%)\n",
      "  Run again with max_depth set to a larger value to avoid saturation\n",
      "None\n",
      "E-BFMI indicated no pathological behavior\n",
      "None\n",
      "0.0 of 3000 iterations ended with a divergence (0.0%)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "diagnose(fit)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b = fit.extract(['alpha'])['alpha'].mean()\n",
    "w = fit.extract(['beta'])['beta'].mean(axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(b)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mean_weights = pd.DataFrame(w)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_features.columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predictions = test_features@w + b"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "numer_test_features = test_features.loc[:, ('volume', 'rolling_mean_week', 'rolling_median_week', \n",
    "                    'rolling_max_week')]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "unscale_test = pd.concat([numer_test_features, predictions],axis=1)\n",
    "unscale_test.columns = ['volume', 'rolling_mean_week', 'rolling_median_week', 'rolling_max_week', 'projected_volume']\n",
    "unscaled_predictions = scaler.inverse_transform(unscale_test)\n",
    "unscale_test.loc[:,:] = unscaled_predictions\n",
    "unscaled_predictions = unscale_test.projected_volume"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_labels_df = pd.Series(test_labels)\n",
    "test_labels_df.index = numer_test_features.index\n",
    "unscale_labels = pd.concat([numer_test_features, test_labels_df],axis=1)\n",
    "unscale_labels.columns = ['volume', 'rolling_mean_week', 'rolling_median_week', 'rolling_max_week', 'projected_volume']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "unscaled_test_labels = scaler.inverse_transform(unscale_labels)\n",
    "unscale_labels.loc[:,:] = unscaled_test_labels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "observed = unscale_labels.projected_volume\n",
    "persistance_prediction = unscale_labels.volume"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = sns.regplot(x=unscaled_predictions, y=observed)\n",
    "fig.set(xlabel='Predicted', ylabel='Observed')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "observ_predict = fig.get_figure()\n",
    "observ_predict.savefig('./figures/observed_vs_predicted.png', dpi=400) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Mean Absolute Percentage Error - Baseline:', round(np.mean(np.abs((test_labels - test_features.volume) / test_labels)) * 100))\n",
    "print('Mean Absolute Percentage Error - Bayes:', round(np.mean(np.abs((test_labels - predictions) / test_labels)) * 100))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Mean Absolute Percentage Error - Baseline:', round(np.mean(np.abs((observed - persistance_prediction) / observed)) * 100))\n",
    "print('Mean Absolute Percentage Error - Bayes:', round(np.mean(np.abs((observed - unscaled_predictions) / observed)) * 100))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "samples = fit.extract()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "beta = samples['beta']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "col_names = test_features.columns\n",
    "weights = pd.DataFrame(beta)\n",
    "weights.columns = col_names\n",
    "melted_weights = weights.melt()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "desired_features = ['volume', 'rolling_mean_week', 'is_retro', 'is_yeezy']\n",
    "posterior_features = melted_weights[melted_weights['variable'].isin(desired_features)]\n",
    "g = sns.FacetGrid(posterior_features, col='variable', sharex=False, sharey=False)\n",
    "g = g.map(sns.distplot, \"value\")\n",
    "g.savefig('./figures/variable_posteriors.png', dpi = 400)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = sns.FacetGrid(melted_weights, col='variable', col_wrap=5, sharex=False, sharey=False)\n",
    "a = a.map(sns.distplot, \"value\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
