{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './most_popular_sneakers/'\n",
    "file_list = [f for f in os.listdir(input_dir) if not f.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jordan_4_2019.txt',\n",
       " 'nike_all_2017.txt',\n",
       " 'jordan_4_2018.txt',\n",
       " 'adidas_nmd_2017.txt',\n",
       " 'jordan_1_2017.txt',\n",
       " 'adidas_yeezy_2018.txt',\n",
       " 'adidas_yeezy_2019.txt',\n",
       " 'jordan_6_2017.txt',\n",
       " 'nike_other_2018.txt',\n",
       " 'jordan_11_2018.txt',\n",
       " 'adidas_ultraboost_2018.txt',\n",
       " 'adidas_ultraboost_2019.txt',\n",
       " 'jordan_11_2019.txt',\n",
       " 'other_2017.txt',\n",
       " 'jordan_11_2017.txt',\n",
       " 'adidas_ultraboost_2017.txt',\n",
       " 'other_2019.txt',\n",
       " 'other_2018.txt',\n",
       " 'jordan_6_2019.txt',\n",
       " 'jordan_6_2018.txt',\n",
       " 'nike_af_2017.txt',\n",
       " 'nike_af_2018',\n",
       " 'jordan_1_2019.txt',\n",
       " 'jordan_1_2018.txt',\n",
       " 'adidas_yeezy_2017.txt',\n",
       " 'nike_all_2018.txt',\n",
       " 'jordan_4_2017.txt',\n",
       " 'nike_all_2019.txt',\n",
       " 'adidas_nmd_2018.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = input_dir + file_list[-5]\n",
    "with open(tmp,'r') as f:\n",
    "        name = [line for line in f if 'name' in line]\n",
    "with open(tmp,'r') as f:\n",
    "    rawDates = [line for line in f if 'releaseDate' in line]\n",
    "    replaceDates = [w.replace(':', '-') for w in rawDates]\n",
    "    formattedDates = [w.replace('releaseDate', '') for w in replaceDates]\n",
    "with open(tmp,'r') as f:\n",
    "    brand = [line for line in f if '\"brand\" :' in line]\n",
    "with open(tmp,'r') as f:\n",
    "    model = [line for line in f if '\"model\" :' in line]\n",
    "with open(tmp,'r') as f:\n",
    "    sku = [line for line in f if '\"sku\" :' in line]\n",
    "with open(tmp,'r') as f:\n",
    "    color = [line for line in f if '\"color\" : ' in line]\n",
    "with open(tmp,'r') as f:\n",
    "    image_url = [line for line in f if '\"image\" : ' in line]\n",
    "# with open(tmp,'r') as f:\n",
    "#     url_dup = [line for line in f if '\"url\"' in line]\n",
    "#     url = [k for k in url_dup if '#' not in k]\n",
    "#url = [k for k in url_dup if '#' not in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stockx(file):\n",
    "    with open(file,'r') as f:\n",
    "        name = [line for line in f if 'name' in line]\n",
    "    with open(file,'r') as f:\n",
    "        rawDates = [line for line in f if 'releaseDate' in line]\n",
    "        replaceDates = [w.replace(':', '-') for w in rawDates]\n",
    "        formattedDates = [w.replace('releaseDate', '') for w in replaceDates]\n",
    "    with open(file,'r') as f:\n",
    "        brand = [line for line in f if 'brand' in line]\n",
    "    with open(file,'r') as f:\n",
    "        model = [line for line in f if 'model' in line]\n",
    "    with open(file,'r') as f:\n",
    "        sku = [line for line in f if 'sku' in line]\n",
    "    with open(file,'r') as f:\n",
    "        color = [line for line in f if '\"color\" : ' in line]\n",
    "    with open(file,'r') as f:\n",
    "        image_url = [line for line in f if '\"image\" : ' in line]\n",
    "#     with open(file,'r') as f:\n",
    "#         url_dup = [line for line in f if '\"url\" : ' in line]\n",
    "#         url = [k for k in url_dup if '#' not in k]\n",
    "    \n",
    "    df = pd.DataFrame({'name':name,\n",
    "            'releaseDate':formattedDates,\n",
    "            'brand':brand,\n",
    "            'model':model,\n",
    "            'sku':sku,\n",
    "            'color':color,\n",
    "            'image':image_url})\n",
    "#                        ,'url':url})\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(df):\n",
    "    df = df.applymap(lambda x: x.replace('\"', ''))\n",
    "    for i in df.columns:\n",
    "        df[str(i)] = df[str(i)].str.split(':').str[-1].str.strip()\n",
    "        df[str(i)] = df[str(i)].str.replace(',', '')\n",
    "    \n",
    "    # creat new column for url scraping\n",
    "    front = 'https://stockx.com/api/products/'\n",
    "    end = '/activity?state=480&currency=USD&limit=20000&page=1&sort=createdAt&order=DESC'\n",
    "    add_https = 'https:'\n",
    "    \n",
    "    df['api_url'] = front + df['sku'] + end\n",
    "    df['image'] = add_https + df['image']\n",
    "#     df['url'] = add_https + df['url']\n",
    "    df['releaseDate'] = df['releaseDate'].str[2:]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputURLs(input_dir):\n",
    "    file_list = [f for f in os.listdir(input_dir) if not f.startswith('.')]\n",
    "    \n",
    "    d = {'name':[],\n",
    "     'releaseDate':[],\n",
    "     'brand':[],\n",
    "     'model':[],\n",
    "     'sku':[],\n",
    "     'color':[],\n",
    "     'api_url':[]}\n",
    "\n",
    "    df = pd.DataFrame(d)\n",
    "    \n",
    "    for file in file_list:\n",
    "        file_str = input_dir + ''.join(file)\n",
    "        data = parse_stockx(file_str)\n",
    "        formatted_df = format_data(data)\n",
    "\n",
    "        brand = file_str.split('/')[-1].split('.')[0]\n",
    "        out_dir = './sneaker_urls/'\n",
    "        out_name = brand + '_urls.csv'\n",
    "        out_path = out_dir + out_name\n",
    "        \n",
    "        formatted_df.api_url.to_csv(out_path, index=False)\n",
    "        \n",
    "        print('URLs for ' + brand + ' have been saved!')\n",
    "\n",
    "        df = pd.concat([df, formatted_df])\n",
    "        print(brand + ' has been successfully appended')\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './most_popular_sneakers/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allenchang/PycharmProjects/insight/venv/lib/python3.7/site-packages/ipykernel_launcher.py:24: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "/Users/allenchang/PycharmProjects/insight/venv/lib/python3.7/site-packages/ipykernel_launcher.py:28: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs for jordan_4_2019 have been saved!\n",
      "jordan_4_2019 has been successfully appended\n",
      "URLs for nike_all_2017 have been saved!\n",
      "nike_all_2017 has been successfully appended\n",
      "URLs for jordan_4_2018 have been saved!\n",
      "jordan_4_2018 has been successfully appended\n",
      "URLs for adidas_nmd_2017 have been saved!\n",
      "adidas_nmd_2017 has been successfully appended\n",
      "URLs for jordan_1_2017 have been saved!\n",
      "jordan_1_2017 has been successfully appended\n",
      "URLs for adidas_yeezy_2018 have been saved!\n",
      "adidas_yeezy_2018 has been successfully appended\n",
      "URLs for adidas_yeezy_2019 have been saved!\n",
      "adidas_yeezy_2019 has been successfully appended\n",
      "URLs for jordan_6_2017 have been saved!\n",
      "jordan_6_2017 has been successfully appended\n",
      "URLs for nike_other_2018 have been saved!\n",
      "nike_other_2018 has been successfully appended\n",
      "URLs for jordan_11_2018 have been saved!\n",
      "jordan_11_2018 has been successfully appended\n",
      "URLs for adidas_ultraboost_2018 have been saved!\n",
      "adidas_ultraboost_2018 has been successfully appended\n",
      "URLs for adidas_ultraboost_2019 have been saved!\n",
      "adidas_ultraboost_2019 has been successfully appended\n",
      "URLs for jordan_11_2019 have been saved!\n",
      "jordan_11_2019 has been successfully appended\n",
      "URLs for other_2017 have been saved!\n",
      "other_2017 has been successfully appended\n",
      "URLs for jordan_11_2017 have been saved!\n",
      "jordan_11_2017 has been successfully appended\n",
      "URLs for adidas_ultraboost_2017 have been saved!\n",
      "adidas_ultraboost_2017 has been successfully appended\n",
      "URLs for other_2019 have been saved!\n",
      "other_2019 has been successfully appended\n",
      "URLs for other_2018 have been saved!\n",
      "other_2018 has been successfully appended\n",
      "URLs for jordan_6_2019 have been saved!\n",
      "jordan_6_2019 has been successfully appended\n",
      "URLs for jordan_6_2018 have been saved!\n",
      "jordan_6_2018 has been successfully appended\n",
      "URLs for nike_af_2017 have been saved!\n",
      "nike_af_2017 has been successfully appended\n",
      "URLs for nike_af_2018 have been saved!\n",
      "nike_af_2018 has been successfully appended\n",
      "URLs for jordan_1_2019 have been saved!\n",
      "jordan_1_2019 has been successfully appended\n",
      "URLs for jordan_1_2018 have been saved!\n",
      "jordan_1_2018 has been successfully appended\n",
      "URLs for adidas_yeezy_2017 have been saved!\n",
      "adidas_yeezy_2017 has been successfully appended\n",
      "URLs for nike_all_2018 have been saved!\n",
      "nike_all_2018 has been successfully appended\n",
      "URLs for jordan_4_2017 have been saved!\n",
      "jordan_4_2017 has been successfully appended\n",
      "URLs for nike_all_2019 have been saved!\n",
      "nike_all_2019 has been successfully appended\n",
      "URLs for adidas_nmd_2018 have been saved!\n",
      "adidas_nmd_2018 has been successfully appended\n"
     ]
    }
   ],
   "source": [
    "df = OutputURLs(input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sku = df.groupby(\"sku\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sku['sku'] = unique_sku.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sku.to_csv('sneaker_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_brands = ['Jordan', 'adidas', 'Nike']\n",
    "\n",
    "ni_ad_jo = unique_sku[unique_sku['brand'].isin(top_brands)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni_ad_jo.to_csv('nike_adidas_jordan_metadata.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
